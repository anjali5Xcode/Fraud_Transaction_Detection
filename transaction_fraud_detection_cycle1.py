# -*- coding: utf-8 -*-
"""transaction-fraud-detection-cycle1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WH9eBGBHckU4OBgRjafhQ9bp1wtG2tWi

# i. Business Understanding

## i.i Blocker Fraud Company

* The Blocker Fraud Company is a company specialized in detecting fraud in financial transactions made through mobile devices. The company has a service called “Blocker Fraud” with no guarantee of blocking fraudulent transactions.

* And the business model of the company is of the Service type with the monetization made by the performance of the service provided, that is, the user pays a fixed fee on the success in detecting fraud in the customer's transactions.

### i.i.i Expansion Problem

Blocker Fraud Company is expanding in Brazil and to acquire customers more quickly, it has adopted a very aggressive strategy. The strategy works as follows:
1. The company will receive 25% of the value of each transaction that is truly detected as fraud.
1. The company will receive 5% of the value of each transaction detected as fraud, but the transaction is truly legitimate.
1. The company will return 100% of the value to the customer, for each transaction detected as legitimate, however the transaction is truly a fraud.

## i.ii The Challenge

You need to deliver to the CEO of Blocker Fraud Company a production model in which your access will be done via API, that is, customers will send their transactions via API so that your model classifies them as fraudulent or legitimate.

### i.ii.i Business Questions

1. What is the model's Precision and Accuracy?
1. How Reliable is the model in classifying transactions as legitimate or fraudulent?
1. What is the Expected Billing by the Company if we classify 100% of transactions with the model?
1. What is the Loss Expected by the Company in case of model failure?
1. What is the Profit Expected by the Blocker Fraud Company when using the model?

# 0.0 Imports and Helper Functions

## 0.1 Imports
"""

!pip install inflection
!pip install boruta
!pip install category_encoders

import joblib
import warnings
import inflection

import numpy             as np
import pandas            as pd
import seaborn           as sns

import matplotlib.pyplot as plt

from scipy   import stats
from boruta  import BorutaPy
from category_encoders import OneHotEncoder

from IPython.display      import Image
from IPython.core.display import HTML

from xgboost  import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.svm          import SVC
from sklearn.dummy        import DummyClassifier
from sklearn.ensemble     import RandomForestClassifier
from sklearn.neighbors    import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics         import balanced_accuracy_score, precision_score, classification_report
from sklearn.metrics         import recall_score, f1_score, make_scorer, cohen_kappa_score
from sklearn.preprocessing   import MinMaxScaler
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold

"""## 0.2 Helper Functions"""

warnings.filterwarnings('ignore')

seed = 42
np.random.seed(seed)

# Commented out IPython magic to ensure Python compatibility.
def jupyter_settings():
#     %matplotlib inline
#     %pylab inline

    sns.set(font_scale=1.6)

    plt.style.use('seaborn-darkgrid')
    plt.rcParams['figure.figsize'] = [25, 12]
    plt.rcParams['font.size'] = 16

    display( HTML('<style>.container { width:100% !important; }</style>'))
    pd.options.display.max_columns = None
    pd.options.display.max_rows = None
    pd.set_option('display.expand_frame_repr', False)

jupyter_settings()

def ml_scores(model_name, y_true, y_pred):

    accuracy = balanced_accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    kappa = cohen_kappa_score(y_true, y_pred)

    return pd.DataFrame({'Balanced Accuracy': np.round(accuracy, 3),
                         'Precision': np.round(precision, 3),
                         'Recall': np.round(recall, 3),
                         'F1': np.round(f1, 3),
                         'Kappa': np.round(kappa, 3)},
                        index=[model_name])

def calcCramerV(x, y):
    cm = pd.crosstab(x, y).values
    n = cm.sum()
    r, k = cm.shape

    if n == 0: # Handle case with no data
        return 0.0 # Return float

    chi2 = stats.chi2_contingency(cm)[0]

    # Calculate expected values for chi2 correction
    expected_cm = stats.chi2_contingency(cm)[3]

    # Handle potential division by zero when correcting chi2
    denominator = (n - 1)
    if denominator == 0:
        chi2corr = 0.0 # Return float
    else:
        # Ensure all terms are floats for calculation
        chi2corr = max(0.0, float(chi2) - (float(k) - 1.0) * (float(r) - 1.0) / float(denominator))


    # Handle potential division by zero in the denominator of Cramer's V
    denominator_v = (min(k - 1.0, r - 1.0)) # Use floats
    if denominator_v == 0.0: # Check against float zero
        return 0.0 # Return float
    else:
        # Ensure all terms are floats for calculation
        return np.sqrt((chi2corr / float(n)) / denominator_v)

def ml_cv_results(model_name, model, x, y, verbose=1):

    '''initial'''
    balanced_accuracies = []
    precisions = []
    recalls = []
    f1s = []
    kappas = []

    mm = MinMaxScaler()

    x_ = x.to_numpy()
    y_ = y.to_numpy()

    count = 0

    '''cross-validation'''
    skf = StratifiedKFold(n_splits=5, shuffle=True)

    for index_train, index_test in skf.split(x_, y_):
        ## Showing the Fold
        if verbose > 0:
            count += 1
            print('Fold K=%i' % (count))

        ## selecting train and test
        x_train, x_test = x.iloc[index_train], x.iloc[index_test]
        y_train, y_test = y.iloc[index_train], y.iloc[index_test]

        ## applying the scale
        x_train = mm.fit_transform(x_train)
        x_test = mm.transform(x_test)

        ## training the model
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)

        ## saving the metrics
        balanced_accuracies.append(balanced_accuracy_score(y_test, y_pred))
        precisions.append(precision_score(y_test, y_pred))
        recalls.append(recall_score(y_test, y_pred))
        f1s.append(f1_score(y_test, y_pred))
        kappas.append(cohen_kappa_score(y_test, y_pred))


    '''results'''
    accuracy_mean, accuracy_std = np.round(np.mean(balanced_accuracies), 3), np.round(np.std(balanced_accuracies), 3)
    precision_mean, precision_std = np.round(np.mean(precisions), 3), np.round(np.std(precisions), 3)
    recall_mean, recall_std = np.round(np.mean(recalls), 3), np.round(np.std(recalls), 3)
    f1_mean, f1_std = np.round(np.mean(f1s), 3), np.round(np.std(f1s), 3)
    kappa_mean, kappa_std = np.round(np.mean(kappas), 3), np.round(np.std(kappas), 3)

    ## saving the results in a dataframe
    return pd.DataFrame({"Balanced Accuracy": "{} +/- {}".format(accuracy_mean, accuracy_std),
                        "Precision": "{} +/- {}".format(precision_mean, precision_std),
                        "Recall": "{} +/- {}".format(recall_mean, recall_std),
                        "F1": "{} +/- {}".format(f1_mean, f1_std),
                        "Kappa": "{} +/- {}".format(kappa_mean, kappa_std)},
                       index=[model_name])

"""# 1.0 Data Description

## 1.1 Loading Data
"""

df1 = pd.read_csv('/content/fraud_0.1origbase.csv')

df1.head()

df1.tail()

"""## 1.2 Columns

### 1.2.1 Column Descriptions

**step:** maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).

**type:** CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.

**amount:** amount of the transaction in local currency.

**nameOrig:** customer who started the transaction

**oldbalanceOrg:** initial balance before the transaction

**newbalanceOrig:** new balance after the transaction

**nameDest:** customer who is the recipient of the transaction

**oldbalanceDest:** initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).

**newbalanceDest:** new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).

**isFraud:** This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.

**isFlaggedFraud:** The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.

### 1.2.2 Column Rename
"""

cols_old = df1.columns.tolist()

snakecase = lambda x: inflection.underscore(x)
cols_new = list(map(snakecase, cols_old))

df1.columns = cols_new

df1.columns

"""## 1.3 Data Dimension"""

print('Number of Rows: {}'.format(df1.shape[0]))
print('Number of Cols: {}'.format(df1.shape[1]))

"""## 1.4 Data Types and Structure"""

df1.info()

"""## 1.5 Check NA"""

df1.isna().mean()

"""## 1.6 Fill Out NA

There's no NaN values to fill.

## 1.7 Change Data Type

I will change the values 0 and 1 to 'yes' and 'no'. It'll help on the data description and analysis sections.
"""

df1['is_fraud'] = df1['is_fraud'].map({1: 'yes', 0: 'no'})
df1['is_flagged_fraud'] = df1['is_flagged_fraud'].map({1: 'yes', 0: 'no'})

"""## 1.8 Description Statistics"""

num_attributes = df1.select_dtypes(exclude='object')
cat_attributes = df1.select_dtypes(include='object')

"""### 1.8.1 Numerical Attributes"""

describe = num_attributes.describe().T

describe['range'] = (num_attributes.max() - num_attributes.min()).tolist()
describe['variation coefficient'] = (num_attributes.std() / num_attributes.mean()).tolist()
describe['skew'] = num_attributes.skew().tolist()
describe['kurtosis'] = num_attributes.kurtosis().tolist()

describe

"""* All the data has a coeficient of variation greater than 25%, therefore they aren't homogeneous.

* The step variable starts from 1 hour to 742 hour (30 days).

* Some variables are higher shap and right skewed.

* 50% of the newbalance_orig is 0. Maybe there are some transfers that don't go to the destination.

* The skew is higher positive, therefore the values may be in less values.

### 1.8.2 Categorical Attributes
"""

cat_attributes.describe()

"""* The majority type is cash_out with 2237500.

* There's a lot of variability in name_orig, so it could be hard to use one hot encoding.

* There's less name_orig than name_dest. There's more users sending than receiving, however use one hot encoding will not help.

* There's more fraud than the flagged fraud, it shows that the current method can't recognize fraud efficiently.

# 2.0 Feature Engineering
"""

df2 = df1.copy()

"""## 2.1 Mind Map"""

Image('/content/Fraud-mindmap.png')

"""## 2.2 Hypothesis Creation

### 2.2.1 User

* 90% of the twentyone-year-old users did a fraud transaction.

* The majority fraud transiction occours for the same initial letter user.

* The fraud amount is greater than 10.000.

* The 60% of the age is greater than 30 year old.

### 2.2.2 Type

* 60% of fraud transaction occours using cash-out-type method.

* The majority transfers occours using tranfers-type method.

* Values greater than 100.000 occours using transfers-type method.

* Payment type occurs with values lower than 100.000

### 2.2.3 Origin and Destiny Transactions

* 60% of the difference between origin destiny transactions is equal 0 for frauds.

* Origin values are greater than destiny values for fraud transaction.

### 2.2.4 Time

* Fraud transactions occours at least in 3 days.

* 40% of the cash-out transactions occours less than 1 day.

* 60% of the transaction less than 100.000 occours at least 10 days.

* The transactions greater than 10.000 occours at most in 2 weeks.

## 2.3 Hipothesys List

1. The majority fraud transiction occours for the same initial letter user.

1. All the fraud amount is greater than 10.000.

1. 60% of fraud transaction occours using cash-out-type method.

1. The majority transfers occours using tranfers-type method.

1. Fraud transactions occours at least in 3 days.

## 2.4 Feature Engineering
"""

# step
df2['step_days'] = df2['step'].apply(lambda i: i/24)
df2['step_weeks'] = df2['step'].apply(lambda i: i/(24*7))

# difference between initial balance before the transaction and new balance after the transaction
df2['diff_new_old_balance'] = df2['newbalance_orig'] - df2['oldbalance_org']

# difference between initial balance recipient before the transaction and new balance recipient after the transaction.
df2['diff_new_old_destiny'] = df2['newbalance_dest'] - df2['oldbalance_dest']

# name orig and name dest
df2['name_orig'] = df2['name_orig'].apply(lambda i: i[0])
df2['name_dest'] = df2['name_dest'].apply(lambda i: i[0])

"""# 3.0 Selecting Columns"""

df3 = df2.copy()

"""## 3.1 Selecting Columns

I'll use all the columns for data analysis

## 3.2 Selecting Lines

I'll use all the lines.

# 4.0 Exploratory Data Analisys
"""

df4 = df3.copy()

"""## 4.1 Univariate Analysis

### 4.1.1 Response Variable
"""

ax = sns.countplot(y='is_fraud', data=df4);

total = df4['is_fraud'].size
for p in ax.patches:
        percentage = ' {:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y))

"""### 4.1.2 Numerical Variables"""

num_attributes = df4.select_dtypes(exclude='object')
columns = num_attributes.columns.tolist()
j = 1

# Adjust the subplot grid based on the number of numerical columns
n_cols = 4
n_rows = (len(columns) + n_cols - 1) // n_cols

plt.figure(figsize=(n_cols * 5, n_rows * 5)) # Adjust figure size
for column in columns:
    plt.subplot(n_rows, n_cols, j)
    sns.histplot(num_attributes[column], kde=True); # Using histplot with kde for distribution

    j += 1

plt.tight_layout() # Adjust layout to prevent overlapping titles
plt.show() # Display the plot

"""### 4.1.3 Categorical Variables"""

cat_attributes = df4.select_dtypes(include='object')
columns = cat_attributes.columns.tolist()
j = 1

# Adjust the subplot grid based on the number of categorical columns
n_cols = 3 # Adjust number of columns as needed
n_rows = (len(columns) + n_cols - 1) // n_cols

plt.figure(figsize=(n_cols * 6, n_rows * 6)) # Adjust figure size
for column in columns:
    plt.subplot(n_rows, n_cols, j)
    ax = sns.countplot(y=column, data=cat_attributes)

    total = cat_attributes[column].size
    for p in ax.patches:
        percentage = ' {:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y))

    j += 1

plt.tight_layout() # Adjust layout to prevent overlapping titles
plt.show() # Display the plot

"""## 4.2 Bivariate Analysis

### H1 The majority fraud transiction occours for the same user.
**TRUE:** The same user origem and destiny has got the same inital letter.
"""

aux1 = df4[df4['is_fraud'] == 'yes']
sns.countplot(y='name_orig', data=aux1);

sns.countplot(y='name_dest', data=aux1);

"""### H2 All the fraud amount is greater than 10.000.

**TRUE:** The values are greater than 10.000. But it's important to note that the no-fraud values is greater than 100.000 also.
"""

sns.barplot(y='amount', x='is_fraud', data=df4);

"""### H3 60% of fraud transaction occours using cash-out-type method.

**FALSE:** The fraud transaction occours in transfer and cash-out type. However they're almost the same value.
"""

aux1 = df4[df4['is_fraud'] == 'yes']
ax = sns.countplot(y='type', data=aux1)

total = aux1['type'].size
for p in ax.patches:
        percentage = ' {:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y))

"""To see the complete transiction-type and I'll plot them here."""

ax = sns.countplot(y='type', hue='is_fraud', data=df4)

total = df4['type'].size
for p in ax.patches:
        percentage = ' {:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y))

# Move the legend to the bottom right corner
ax.legend(loc='lower right', title='is_fraud')

"""### H4 Values greater than 100.000 occours using transfers-type method.

**FALSE:** The majority transactions occours in trasnfer-type, however transactions greater than 100.000 occour in cash-out and cash-in too.
"""

ax = sns.barplot(y='type', x='amount', data=df4);

total = df4['type'].size
for p in ax.patches:
        percentage = ' {:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y))

"""### H5 Fraud transactions occours at least in 3 days.

**TRUE:** The values for transactions and days in fraud aren't similar.
"""

aux1 = df4[df4['is_fraud'] == 'yes']
sns.regplot(x='step_days', y='amount', data=aux1);

"""## 4.3 Multivariaty Analysis

### 4.3.1 Numerical Analysis
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Correlation matrix
corr = num_attributes.corr()

# Mask upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set figure size
plt.figure(figsize=(10, 6))

# Heatmap
sns.heatmap(
    corr,
    mask=mask,
    annot=True,
    fmt=".2f",        # show only 2 decimal points
    cmap="coolwarm",  # better color scheme
    vmin=-1,
    vmax=1,
    center=0,
    square=True,
    cbar_kws={"shrink": 0.8},  # shrink colorbar
    annot_kws={"size": 8}      # smaller font size
)

plt.title("Correlation Heatmap of Numerical Features", fontsize=14, pad=20)
plt.xticks(rotation=45, ha="right", fontsize=9)
plt.yticks(rotation=0, fontsize=9)
plt.tight_layout()
plt.show()

"""### 4.3.2 Categorical Variables

# 5.0 Data Preparation
"""

df5 = df4.copy()

"""## 5.1 Spliting into Train, Valid and Test"""

X = df5.drop(columns=['is_fraud', 'is_flagged_fraud', 'name_orig', 'name_dest',
                      'step_weeks', 'step_days'], axis=1)
y = df5['is_fraud'].map({'yes': 1, 'no': 0})

# spliting into temp and test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=.2, stratify=y)

# spliting into train and valid
X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=.2, stratify=y_temp)

"""## 5.2 One Hot Encoder"""

ohe = OneHotEncoder(cols=['type'], use_cat_names=True)

X_train = ohe.fit_transform(X_train)
X_valid = ohe.transform(X_valid)

X_temp = ohe.fit_transform(X_temp)
X_test = ohe.transform(X_test)

"""## 5.3 Rescaling"""

num_columns = ['amount', 'oldbalance_org', 'newbalance_orig', 'oldbalance_dest', 'newbalance_dest',
               'diff_new_old_balance', 'diff_new_old_destiny']
mm = MinMaxScaler()
X_params = X_temp.copy()

X_train[num_columns] = mm.fit_transform(X_train[num_columns])
X_valid[num_columns] = mm.transform(X_valid[num_columns])

X_params[num_columns] = mm.fit_transform(X_temp[num_columns])
X_test[num_columns] = mm.transform(X_test[num_columns])

"""# 6.0 Feature Selection

## 6.1 Boruta
"""

# X_boruta = X_params.values
# y_boruta = y_temp.values.ravel()

# boruta = BorutaPy(RandomForestClassifier(), n_estimators='auto')
# boruta.fit(X_boruta, y_boruta)

"""### 6.1.1 Best Features"""

# cols_selected_boruta = boruta.support_.tolist()

# columns_selected = X_params.loc[:, cols_selected_boruta].columns.tolist()

# columns_selected

# ['step',
#  'amount',
#  'oldbalance_org',
#  'newbalance_orig',
#  'oldbalance_dest',
#  'newbalance_dest',
#  'diff_new_old_balance',
#  'diff_new_old_destiny',
#  'type_TRANSFER']

final_columns_selected = ['step', 'oldbalance_org',
                          'newbalance_orig', 'newbalance_dest',
                          'diff_new_old_balance', 'diff_new_old_destiny',
                          'type_TRANSFER']

"""# 7.0 Machine Learning Modeling"""

X_train_cs = X_train[final_columns_selected]
X_valid_cs = X_valid[final_columns_selected]

X_temp_cs = X_temp[final_columns_selected]
X_test_cs = X_test[final_columns_selected]

X_params_cs = X_params[final_columns_selected]

"""## 7.1 Baseline"""

dummy = DummyClassifier()
dummy.fit(X_train_cs, y_train)

y_pred = dummy.predict(X_valid_cs)

dummy_results = ml_scores('dummy', y_valid, y_pred)
dummy_results

"""### 7.1.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.1.2 Cross Validation"""

dummy_cv = ml_cv_results('Dummy', DummyClassifier(), X_temp, y_temp)
dummy_cv

"""## 7.2 Logistic Regression"""

lg = LogisticRegression()
lg.fit(X_train_cs, y_train)

y_pred = lg.predict(X_valid_cs)

lg_results = ml_scores('Logistic Regression', y_valid, y_pred)
lg_results

"""### 7.2.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.2.2 Cross Validation"""

lg_cv = ml_cv_results('Logistic Regression',
                      LogisticRegression(),
                      X_temp_cs, y_temp)
lg_cv

"""## 7.3 K Nearest Neighbors"""

knn = KNeighborsClassifier()
knn.fit(X_train_cs, y_train)

y_pred = knn.predict(X_valid_cs)

knn_results = ml_scores('K Nearest Neighbors', y_valid, y_pred)
knn_results

"""### 7.3.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.3.2 Cross Validation"""

knn_cv = ml_cv_results('K Nearest Neighbors', KNeighborsClassifier(),
                       X_temp_cs, y_temp)
knn_cv

"""## 7.4 Support Vector Machine"""

svm = SVC()
svm.fit(X_train_cs, y_train)

y_pred = svm.predict(X_valid_cs)

svm_results = ml_scores('SVM', y_valid, y_pred)
svm_results

"""### 7.4.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.4.2 Cross Validation"""

svm_cv = ml_cv_results('SVM', SVC(), X_temp_cs, y_temp)
svm_cv

"""## 7.5 Random Forest"""

rf = RandomForestClassifier(class_weight='balanced')
rf.fit(X_train_cs, y_train)

y_pred = rf.predict(X_valid_cs)

rf_results = ml_scores('Random Forest', y_valid, y_pred)
rf_results

"""### 7.5.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.5.2 Cross Validation"""

rf_cv = ml_cv_results('Random Forest',
                      RandomForestClassifier(),
                      X_temp_cs, y_temp)
rf_cv

"""## 7.6 XGBoost"""

xgb = XGBClassifier()
xgb.fit(X_train_cs, y_train)

y_pred = xgb.predict(X_valid_cs)

xgb_results = ml_scores('XGBoost', y_valid, y_pred)
xgb_results

"""### 7.6.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.6.2 Cross Validation"""

xgb_cv = ml_cv_results('XGBoost', XGBClassifier(),
                       X_temp_cs, y_temp)
xgb_cv

"""## 7.7 LightGBM"""

lightgbm = LGBMClassifier()
lightgbm.fit(X_train_cs, y_train)

y_pred = lightgbm.predict(X_valid_cs)

lightgbm_results = ml_scores('LightGBM', y_valid, y_pred)
lightgbm_results

"""### 7.7.1 Classification Report"""

print(classification_report(y_valid, y_pred))

"""### 7.7.2 Cross Validation"""

lightgbm_cv = ml_cv_results('LightGDM', LGBMClassifier(),
                            X_temp_cs, y_temp)
lightgbm_cv

"""## 7.8 Comparing Model's Performance

### 7.8.1 Single Performance
"""

modeling_performance = pd.concat([dummy_results, lg_results, knn_results,
                                  rf_results, xgb_results, lightgbm_results,
                                 svm_results])
modeling_performance.sort_values(by="F1", ascending=True)

"""### 7.8.2 Cross Validation Performance"""

modeling_performance_cv = pd.concat([dummy_cv, lg_cv, knn_cv, rf_cv,
                                     xgb_cv, lightgbm_cv, svm_cv])

modeling_performance_cv.sort_values(by="F1", ascending=True)

"""# 8.0 Hyperparameter Fine Tuning"""

f1 = make_scorer(f1_score)

params = {
    'booster': ['gbtree', 'gblinear', 'dart'],
    'eta': [0.3, 0.1, 0.01],
    'scale_pos_weight': [1, 774, 508, 99]
}

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold

# Parameter space (not too big to avoid long runtime)
params = {
    "n_estimators": [200, 400, 600],
    "max_depth": [3, 5, 7, 10],
    "learning_rate": [0.01, 0.05, 0.1],
    "subsample": [0.7, 0.9, 1],
    "colsample_bytree": [0.7, 0.9, 1],
    "scale_pos_weight": [1, 5, 10, 20]
}

xgb = XGBClassifier(
    use_label_encoder=False,
    eval_metric="logloss",
    random_state=42
)

# Use RandomizedSearch instead of GridSearch
search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=params,
    n_iter=10,  # only test 10 random combos
    scoring="f1",
    cv=StratifiedKFold(n_splits=3),
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# ⚡ Use a sample of the dataset for speed
X_sample = X_params_cs.sample(20000, random_state=42)
y_sample = y_temp.loc[X_sample.index]

# Fit RandomizedSearch
search.fit(X_sample, y_sample)

print("Best parameters:", search.best_params_)

# Train the final model on full dataset with best params
best_model = XGBClassifier(
    **search.best_params_,
    use_label_encoder=False,
    eval_metric="logloss",
    random_state=42
)
best_model.fit(X_params_cs, y_temp)

best_params = search.best_params_
best_params

best_params = {'booster': 'gbtree', 'eta': 0.3, 'scale_pos_weight': 1}

search.best_score_

"""## 8.1 Results"""

xgb_gs = XGBClassifier(
    booster=best_params['booster'],
    eta=best_params['eta'],
    scale_pos_weight=best_params['scale_pos_weight']
)

search.fit(X_train_cs, y_train)

y_pred = search.predict(X_valid_cs)

"""### 8.1.2 Single Results"""

xgb_rs_results = ml_scores('XGBoost RS', y_valid, y_pred)
xgb_rs_results

"""### 8.1.3 Cross Validation"""

xgb_rs_cv = ml_cv_results('XGBoost RS', search.best_estimator_, X_temp_cs, y_temp)
xgb_rs_cv

"""# 9.0 Conclusions

## 9.1 Final Model
"""

final_model = XGBClassifier(
    booster=best_params['booster'],
    eta=best_params['eta'],
    scale_pos_weight=best_params['scale_pos_weight']
)

final_model.fit(X_params_cs, y_temp)

"""### 9.1.1 Unseen Data Score"""

y_pred = final_model.predict(X_test_cs)

unseen_scores = ml_scores('unseen', y_test, y_pred)
unseen_scores

"""## 9.2 Blocker Fraud Company Expasion

### 9.2.1 The company receives 25% of each transaction value truly detected as fraud.
"""

df_test = df5.loc[X_test.index, :]
df_test['predictions'] = y_pred

aux1 = df_test[(df_test['is_fraud'] == 'yes') & (df_test['predictions'] == 1)]
receives = aux1['amount'].sum() * 0.25

print('The company can receive %.2f detecting fraud transactions.' % (receives))

"""### 9.2.2 The company receives 5% of each transaction value detected as fraud, however the transaction is legitimate."""

aux1 = df_test[(df_test['is_fraud'] == 'no') & (df_test['predictions'] == 1)]
receives = aux1['amount'].sum() * 0.05

print('For wrong decisions, the company can receive %.2f.' % (receives))

"""### 9.2.3 The company gives back 100% of the value for the customer in each transaction detected as legitimate, however the transaction is actually a fraud."""

aux1 = df_test[(df_test['is_fraud'] == 'yes') & (df_test['predictions'] == 0)]
receives = aux1['amount'].sum()

print('However, the company must return the amount of %.2f.' % (receives))

"""## 9.3 Model's Performance

### 9.3.1 What is the model's Precision and Accuracy?
"""

print('For unseen data, the values of balanced accuracy is equal %.2f and precision is equal %.2f.' % (unseen_scores['Balanced Accuracy'], unseen_scores['Precision']))

"""### 9.3.2 How reliable is the model in classifying transactions as legitimate or fraudulent?"""

print('The model can detect 0.851 +/- 0.023 of the fraud. However it detected 0.84 of the frauds from a unseen data.')

"""### 9.3.3 What is the revenue expected by the company  classify 100% of transactions with the model?"""

aux1 = df_test[(df_test['is_fraud'] == 'yes') & (df_test['predictions'] == 1)]
receives = aux1['amount'].sum() * 0.25

aux2 = df_test[(df_test['is_fraud'] == 'no') & (df_test['predictions'] == 1)]
receives2 = aux2['amount'].sum() * 0.05

print('Using the model the company can revenue %.2f.' % (receives + receives2))

aux3 = df_test[(df_test['is_fraud'] == 'yes') & (df_test['is_flagged_fraud'] == 'yes')]
curr_receives = aux3['amount'].sum() * 0.25

aux4 = df_test[(df_test['is_fraud'] == 'no') & (df_test['is_flagged_fraud'] == 'yes')]
curr_receives2 = aux4['amount'].sum() * 0.05

print('However the currently method the revenue is %.2f.' % (curr_receives + curr_receives2))

"""### 9.3.4 What is the loss expected by the Company if it classifies 100% of the transactions with the model?"""

aux1 = df_test[(df_test['is_fraud'] == 'yes') & (df_test['predictions'] == 0)]
loss = aux1['amount'].sum()

print('For wrong classifications the company must return the amount of %.2f.' % (loss))

aux1 = df_test[(df_test['is_fraud'] == 'yes') & (df_test['is_flagged_fraud'] == 'no')]
curr_loss = aux1['amount'].sum()

print('For wrong classifications using the currently method, the company must return the amount of %.2f.' % (curr_loss))

"""### 9.3.5 What is the profit expected by the blocker fraud company when using the model?"""

print('The company can expect the profit of %.2f.' % (receives + receives2 - loss))

print('Using the currently method, the profit is %.2f.' % (curr_receives + curr_receives - curr_loss))

"""# 10.0 Model Deploy

## 10.1 Saving
"""

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

minmaxscaler = MinMaxScaler()
onehotencoder = OneHotEncoder(handle_unknown='ignore')

import joblib

# Save scaler and encoder
joblib.dump(minmaxscaler, "minmaxscaler_cycle1.joblib")
joblib.dump(onehotencoder, "onehotencoder_cycle1.joblib")

final_model = XGBClassifier(
    booster=best_params['booster'],
    eta=best_params['eta'],
    scale_pos_weight=best_params['scale_pos_weight']
)

final_model.fit(X_params_cs, y_temp)

joblib.dump(final_model, 'fraud_model.pkl')

"""## 10.2 Fraud Class"""

import joblib
import inflection
import pandas as pd

class Fraud:

    def __init__(self):
        # Load pre-trained scalers/encoders
        self.minmaxscaler = joblib.load("minmaxscaler_cycle1.joblib")
        self.onehotencoder = joblib.load("onehotencoder_cycle1.joblib")

        # Define the numeric columns used during training
        self.num_columns = [
            "amount", "oldbalance_org", "newbalance_orig",
            "oldbalance_dest", "newbalance_dest",
            "diff_new_old_balance", "diff_new_old_destiny"
        ]

        # Columns selected in final training
        self.final_columns_selected = [
            "step", "oldbalance_org", "newbalance_orig",
            "newbalance_dest", "diff_new_old_balance",
            "diff_new_old_destiny", "type_TRANSFER"
        ]

    def data_cleaning(self, df):
        """Convert column names to snake_case."""
        cols_old = df.columns.tolist()
        snakecase = lambda i: inflection.underscore(i)
        cols_new = list(map(snakecase, cols_old))
        df.columns = cols_new
        return df

    def feature_engineering(self, df):
        """Add derived features like balance differences."""
        df["step_days"] = df["step"] / 24
        df["step_weeks"] = df["step"] / (24 * 7)

        df["diff_new_old_balance"] = df["newbalance_orig"] - df["oldbalance_org"]
        df["diff_new_old_destiny"] = df["newbalance_dest"] - df["oldbalance_dest"]

        df["name_orig"] = df["name_orig"].apply(lambda i: i[0])
        df["name_dest"] = df["name_dest"].apply(lambda i: i[0])

        # Drop unnecessary columns
        return df.drop(columns=["name_orig", "name_dest", "step_weeks", "step_days"], axis=1)

    def data_preparation(self, df):
        """Apply scaling + encoding in the same way as training."""
        # Scale numeric columns
        df[self.num_columns] = self.minmaxscaler.transform(df[self.num_columns])

        # Apply one-hot encoding
        df_encoded = pd.DataFrame(
            self.onehotencoder.transform(df).toarray(),
            columns=self.onehotencoder.get_feature_names_out(df.columns)
        )

        # Align with training columns
        for col in self.final_columns_selected:
            if col not in df_encoded.columns:
                df_encoded[col] = 0  # add missing columns

        return df_encoded[self.final_columns_selected]

    def get_prediction(self, model, original_data, test_data):
        """Generate predictions and return DataFrame with results."""
        pred = model.predict(test_data)
        original_data = original_data.copy()
        original_data["prediction"] = pred
        return original_data

"""## 10.3 API Handler

## 10.4 API Tester
"""

import streamlit as st
import pandas as pd
import joblib
import inflection

# -------------------------------
# Fraud Class Definition
# -------------------------------
class Fraud:

    def __init__(self):
        # Load pre-trained scalers/encoders
        self.minmaxscaler = joblib.load("minmaxscaler_cycle1.joblib")
        self.onehotencoder = joblib.load("onehotencoder_cycle1.joblib")

        # Define the numeric columns used during training
        self.num_columns = [
            "amount", "oldbalance_org", "newbalance_orig",
            "oldbalance_dest", "newbalance_dest",
            "diff_new_old_balance", "diff_new_old_destiny"
        ]

        # Columns selected in final training
        self.final_columns_selected = [
            "step", "oldbalance_org", "newbalance_orig",
            "newbalance_dest", "diff_new_old_balance",
            "diff_new_old_destiny", "type_TRANSFER"
        ]

    def data_cleaning(self, df):
        """Convert column names to snake_case."""
        cols_old = df.columns.tolist()
        snakecase = lambda i: inflection.underscore(i)
        cols_new = list(map(snakecase, cols_old))
        df.columns = cols_new
        return df

    def feature_engineering(self, df):
        """Add derived features like balance differences."""
        df["step_days"] = df["step"] / 24
        df["step_weeks"] = df["step"] / (24 * 7)

        df["diff_new_old_balance"] = df["newbalance_orig"] - df["oldbalance_org"]
        df["diff_new_old_destiny"] = df["newbalance_dest"] - df["oldbalance_dest"]

        df["name_orig"] = df["name_orig"].apply(lambda i: i[0])
        df["name_dest"] = df["name_dest"].apply(lambda i: i[0])

        # Drop unnecessary columns
        return df.drop(columns=["name_orig", "name_dest", "step_weeks", "step_days"], axis=1)

    def data_preparation(self, df):
        """Apply scaling + encoding in the same way as training."""
        # Scale numeric columns
        df[self.num_columns] = self.minmaxscaler.transform(df[self.num_columns])

        # Apply one-hot encoding
        df_encoded = pd.DataFrame(
            self.onehotencoder.transform(df).toarray(),
            columns=self.onehotencoder.get_feature_names_out(df.columns)
        )

        # Align with training columns - ensure all selected columns are present
        for col in self.final_columns_selected:
            if col not in df_encoded.columns:
                df_encoded[col] = 0  # add missing columns

        return df_encoded[self.final_columns_selected]

    def get_prediction(self, model, original_data, test_data):
        """Generate predictions and return DataFrame with results."""
        pred = model.predict(test_data)
        original_data = original_data.copy()
        original_data["prediction"] = pred
        return original_data


# -------------------------------
# Streamlit App
# -------------------------------

# Load trained model
model = joblib.load("fraud_model.pkl")
pipeline = Fraud()

# App Title
st.title("💳 Transaction Fraud Detection")

# Sidebar Inputs
st.sidebar.header("Enter Transaction Details")
step = st.sidebar.number_input("Step (time unit of transaction)", min_value=1, step=1)
amount = st.sidebar.number_input("Transaction Amount", min_value=0.0, step=0.01)
oldbalance_org = st.sidebar.number_input("Old Balance (Sender)", min_value=0.0, step=0.01)
newbalance_orig = st.sidebar.number_input("New Balance (Sender)", min_value=0.0, step=0.01)
oldbalance_dest = st.sidebar.number_input("Old Balance (Receiver)", min_value=0.0, step=0.01)
newbalance_dest = st.sidebar.number_input("New Balance (Receiver)", min_value=0.0, step=0.01)
transaction_type = st.sidebar.selectbox("Transaction Type", ["CASH_OUT", "PAYMENT", "TRANSFER", "DEBIT", "CASH_IN"])

# Mock IDs (required by pipeline, but not important for prediction)
name_orig = "C123456"
name_dest = "M123456"

# Prediction Button
if st.sidebar.button("Predict Fraud"):
    # Create DataFrame similar to training data
    input_data = pd.DataFrame([{
        "step": step,
        "type": transaction_type,
        "amount": amount,
        "oldbalanceOrg": oldbalance_org,
        "newbalanceOrig": newbalance_orig,
        "oldbalanceDest": oldbalance_dest,
        "newbalanceDest": newbalance_dest,
        "nameOrig": name_orig,
        "nameDest": name_dest
    }])

    # Apply Fraud pipeline
    df1 = pipeline.data_cleaning(input_data)
    df2 = pipeline.feature_engineering(df1)
    df3 = pipeline.data_preparation(df2)
    prediction_df = pipeline.get_prediction(model, input_data, df3)

    # Get prediction result
    pred = prediction_df["prediction"].iloc[0]
    result = "🚨 Fraudulent Transaction" if pred == 1 else "✅ Legitimate Transaction"

    # Show result
    st.subheader("Prediction Result:")
    st.success(result)

    # Optional: show processed input
    with st.expander("🔍 Processed Input Data"):
        st.write(prediction_df)

# loading test dataset
df10 = pd.read_csv("/fraud_0.1origbase.csv").iloc[5, :-2]

df10.head()

!pip install streamlit



!jupyter nbconvert --to python /content/fraud_detection_notebook.ipynb

from google.colab import files

files.download("fraud_model.pkl")
files.download("minmaxscaler_cycle1.joblib")
files.download("onehotencoder_cycle1.joblib")